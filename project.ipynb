{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471bd61e-2563-4add-b00d-f3507fea3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\254mu\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n",
      "Downloading groq-0.13.0-py3-none-any.whl (108 kB)\n",
      "   ---------------------------------------- 0.0/108.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/108.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/108.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/108.8 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/108.8 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/108.8 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/108.8 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 30.7/108.8 kB 130.4 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 30.7/108.8 kB 130.4 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 30.7/108.8 kB 130.4 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 41.0/108.8 kB 103.4 kB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 61.4/108.8 kB 142.2 kB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 61.4/108.8 kB 142.2 kB/s eta 0:00:01\n",
      "   ------------------------- ------------- 71.7/108.8 kB 140.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 92.2/108.8 kB 168.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 108.8/108.8 kB 180.1 kB/s eta 0:00:00\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# pip install anthropic\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed.\n",
      "Page 2 processed.\n",
      "Page 3 processed.\n",
      "Page 4 processed.\n",
      "Page 5 processed.\n",
      "Page 6 processed.\n",
      "Page 7 processed.\n",
      "Page 8 processed.\n",
      "Page 9 processed.\n",
      "Page 10 processed.\n",
      "Page 11 processed.\n",
      "Page 12 processed.\n",
      "Page 13 processed.\n",
      "Page 14 processed.\n",
      "Page 15 processed.\n",
      "Page 16 processed.\n",
      "Page 17 processed.\n",
      "Page 18 processed.\n",
      "Page 19 processed.\n",
      "Page 20 processed.\n",
      "Page 21 processed.\n",
      "Page 22 processed.\n",
      "Page 23 processed.\n",
      "Page 24 processed.\n",
      "Page 25 processed.\n",
      "Page 26 processed.\n",
      "Page 27 processed.\n",
      "Page 28 processed.\n",
      "Page 29 processed.\n",
      "Page 30 processed.\n",
      "Page 31 processed.\n",
      "Page 32 processed.\n",
      "Page 33 processed.\n",
      "Page 34 processed.\n",
      "Page 35 processed.\n",
      "Page 36 processed.\n",
      "Page 37 processed.\n",
      "Page 38 processed.\n",
      "Page 39 processed.\n",
      "Page 40 processed.\n",
      "Page 41 processed.\n",
      "Page 42 processed.\n",
      "Page 43 processed.\n",
      "Page 44 processed.\n",
      "Page 45 processed.\n",
      "Page 46 processed.\n",
      "Page 47 processed.\n",
      "Page 48 processed.\n",
      "Page 49 processed.\n",
      "Page 50 processed.\n",
      "Page 51 processed.\n",
      "Page 52 processed.\n",
      "Page 53 processed.\n",
      "Page 54 processed.\n",
      "Page 55 processed.\n",
      "Page 56 processed.\n",
      "Page 57 processed.\n",
      "Page 58 processed.\n",
      "Page 59 processed.\n",
      "Page 60 processed.\n",
      "Page 61 processed.\n",
      "Page 62 processed.\n",
      "Page 63 processed.\n",
      "Page 64 processed.\n",
      "Page 65 processed.\n",
      "Page 66 processed.\n",
      "Page 67 processed.\n",
      "Page 68 processed.\n",
      "Page 69 processed.\n",
      "Page 70 processed.\n",
      "Page 71 processed.\n",
      "Page 72 processed.\n",
      "Page 73 processed.\n",
      "Page 74 processed.\n",
      "Page 75 processed.\n",
      "Page 76 processed.\n",
      "Page 77 processed.\n",
      "Page 78 processed.\n",
      "Page 79 processed.\n",
      "Page 80 processed.\n",
      "Page 81 processed.\n",
      "Page 82 processed.\n",
      "Page 83 processed.\n",
      "Page 84 processed.\n",
      "Page 85 processed.\n",
      "Page 86 processed.\n",
      "Page 87 processed.\n",
      "Page 88 processed.\n",
      "Page 89 processed.\n",
      "Page 90 processed.\n",
      "Page 91 processed.\n",
      "Page 92 processed.\n",
      "Page 93 processed.\n",
      "Page 94 processed.\n",
      "Page 95 processed.\n",
      "Page 96 processed.\n",
      "Page 97 processed.\n",
      "Page 98 processed.\n",
      "Page 99 processed.\n",
      "Page 100 processed.\n",
      "Page 101 processed.\n",
      "Page 102 processed.\n",
      "Page 103 processed.\n",
      "Page 104 processed.\n",
      "Page 105 processed.\n",
      "Page 106 processed.\n",
      "Page 107 processed.\n",
      "Page 108 processed.\n",
      "Page 109 processed.\n",
      "Page 110 processed.\n",
      "Page 111 processed.\n",
      "Page 112 processed.\n",
      "Page 113 processed.\n",
      "Page 114 processed.\n",
      "Page 115 processed.\n",
      "Page 116 processed.\n",
      "Page 117 processed.\n",
      "Page 118 processed.\n",
      "Page 119 processed.\n",
      "Page 120 processed.\n",
      "Page 121 processed.\n",
      "Page 122 processed.\n",
      "Page 123 processed.\n",
      "Page 124 processed.\n",
      "Page 125 processed.\n",
      "Page 126 processed.\n",
      "Page 127 processed.\n",
      "Page 128 processed.\n",
      "Page 129 processed.\n",
      "Page 130 processed.\n",
      "Page 131 processed.\n",
      "Page 132 processed.\n",
      "Page 133 processed.\n",
      "Page 134 processed.\n",
      "Page 135 processed.\n",
      "Page 136 processed.\n",
      "Page 137 processed.\n",
      "Page 138 processed.\n",
      "Page 139 processed.\n",
      "Page 140 processed.\n",
      "Page 141 processed.\n",
      "Page 142 processed.\n",
      "Page 143 processed.\n",
      "Page 144 processed.\n",
      "Page 145 processed.\n",
      "Page 146 processed.\n",
      "Page 147 processed.\n",
      "Page 148 processed.\n",
      "Page 149 processed.\n",
      "Page 150 processed.\n",
      "Page 151 processed.\n",
      "Page 152 processed.\n",
      "Page 153 processed.\n",
      "Page 154 processed.\n",
      "Page 155 processed.\n",
      "Page 156 processed.\n",
      "Page 157 processed.\n",
      "Page 158 processed.\n",
      "Page 159 processed.\n",
      "Page 160 processed.\n",
      "Page 161 processed.\n",
      "Page 162 processed.\n",
      "Page 163 processed.\n",
      "Page 164 processed.\n",
      "Page 165 processed.\n",
      "Page 166 processed.\n",
      "Page 167 processed.\n",
      "Page 168 processed.\n",
      "Page 169 processed.\n",
      "Page 170 processed.\n",
      "Page 171 processed.\n",
      "Page 172 processed.\n",
      "Page 173 processed.\n",
      "Page 174 processed.\n",
      "Page 175 processed.\n",
      "Page 176 processed.\n",
      "Page 177 processed.\n",
      "Page 178 processed.\n",
      "Page 179 processed.\n",
      "Page 180 processed.\n",
      "Page 181 processed.\n",
      "Page 182 processed.\n",
      "Page 183 processed.\n",
      "Page 184 processed.\n",
      "Page 185 processed.\n",
      "Page 186 processed.\n",
      "Page 187 processed.\n",
      "Page 188 processed.\n",
      "Page 189 processed.\n",
      "Page 190 processed.\n",
      "Page 191 processed.\n",
      "Page 192 processed.\n",
      "Page 193 processed.\n",
      "Page 194 processed.\n",
      "Page 195 processed.\n",
      "Page 196 processed.\n",
      "Page 197 processed.\n",
      "Page 198 processed.\n",
      "Page 199 processed.\n",
      "Page 200 processed.\n",
      "Page 201 processed.\n",
      "Page 202 processed.\n",
      "Page 203 processed.\n",
      "Page 204 processed.\n",
      "Page 205 processed.\n",
      "Page 206 processed.\n",
      "Page 207 processed.\n",
      "Page 208 processed.\n",
      "All data has been saved to 'reviews_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "output_file = 'reviews_data.csv'\n",
    "\n",
    "\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    url = \"https://www.opentable.com/r/north-square-new-york?corrid=2c1ae06f-aca7-47c0-9669-84a28a85a8f4&avt=eyJ2IjoyLCJtIjowLCJwIjowLCJzIjowLCJuIjowfQ&p=1&sd=2024-12-07T19%3A00%3A00&page=1\"\n",
    "    driver.get(url)\n",
    "    restaurantName = driver.find_element(By.XPATH, '//*[@id=\"mainContent\"]/div/div[2]/div[1]/section[1]/div[1]/div/div[1]/h1')\n",
    "    nameText = restaurantName.text\n",
    "    writer.writerow([nameText])\n",
    "    driver.quit()\n",
    "    time.sleep(2)\n",
    "    writer.writerow([\"Review\", \"Rating\", \"Date\"])  # Write header\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    # Loop through pages 1 to 208\n",
    "    for page in range(1, 209):\n",
    "        url = f\"https://www.opentable.com/r/north-square-new-york?corrid=2c1ae06f-aca7-47c0-9669-84a28a85a8f4&avt=eyJ2IjoyLCJtIjowLCJwIjowLCJzIjowLCJuIjowfQ&p={page}&sd=2024-12-07T19%3A00%3A00&page={page}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Allow page to load\n",
    "        \n",
    "        # Extract review content\n",
    "        reviews = driver.find_elements(By.XPATH, '//*[@id=\"restProfileReviewsContent\"]/li/div/div[2]/span[1]')\n",
    "        ratings = driver.find_elements(By.XPATH, '//*[@id=\"restProfileReviewsContent\"]/li/div/ol/li[1]/span')\n",
    "        dates = driver.find_elements(By.XPATH, '//*[@id=\"restProfileReviewsContent\"]/li/div/div[1]/p')\n",
    "        \n",
    "        # Ensure data alignment by zipping elements\n",
    "        for review, rating, date in zip(reviews, ratings, dates):\n",
    "            review_text = review.text\n",
    "            rating_text = rating.text\n",
    "            date_text = date.text\n",
    "            \n",
    "            if review_text and rating_text and date_text:\n",
    "                writer.writerow([review_text, rating_text, date_text])\n",
    "            \n",
    "        print(f\"Page {page} processed.\")\n",
    "    \n",
    "driver.quit()\n",
    "print(f\"All data has been saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a274e3-b987-4906-aaeb-f27549b12989",
   "metadata": {},
   "source": [
    "### Setup your own API key\n",
    "* Login & setup your API key from here (https://console.anthropic.com/settings/keys)\n",
    "* Paste your API key in the following inverted commas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932a01b-c699-44bb-bc1a-4021fb4c7423",
   "metadata": {},
   "source": [
    "***Following code is just a sample, your task is to encapsulate this code in a function with arguments of your choice so that function can be reused***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4094679c-9f31-4b86-9e54-1fd6aaef5c14",
   "metadata": {},
   "source": [
    "#### Following link is for reference: https://docs.anthropic.com/en/docs/initial-setup#next-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83650e80-fcd5-47c7-be5b-77ce16689823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b249df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # Assuming this imports your LLM client\n",
    "\n",
    "# Load the reviews from the CSV file\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for i, row in enumerate(reader):\n",
    "            reviews.append(row[0])  # Assuming reviews are in the first column\n",
    "    return reviews\n",
    "\n",
    "# Send prompt to LLM and extract the response\n",
    "def analyze_review(review_text, category):\n",
    "    prompt = f\"Extract and provide only the comments about {category} from this review: {review_text} , without any introductory or explanatory text. Don't include personal information. \"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "# Process the first 10 reviews and categorize the comments\n",
    "def process_reviews(file_path):\n",
    "    reviews = load_reviews(file_path)\n",
    "    food_comments = []\n",
    "    staff_comments = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        food_comment = analyze_review(review, 'food quality')\n",
    "        staff_comment = analyze_review(review, 'staff/service')\n",
    "        \n",
    "        # Ensure no hallucinations (empty responses)\n",
    "        if food_comment and \"food\" in food_comment.lower():\n",
    "            food_comments.append(food_comment)\n",
    "        if staff_comment and \"staff\" in staff_comment.lower():\n",
    "            staff_comments.append(staff_comment)\n",
    "    \n",
    "    return food_comments, staff_comments\n",
    "\n",
    "def save_to_json(food_comments, staff_comments, output_file=\"categorized_reviews.json\"):\n",
    "    data = {\n",
    "        \"food_comments\": food_comments,\n",
    "        \"staff_comments\": staff_comments\n",
    "    }\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example: Process the first 10 reviews and store the categorized data\n",
    "food_comments, staff_comments = process_reviews(\"reviews_data.csv\")\n",
    "\n",
    "save_to_json(food_comments, staff_comments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc311c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and saved to sentiment_reviews.json.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "def load_reviews(file_path):\n",
    "    \"\"\"Load reviews from a JSON file.\"\"\"\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data  \n",
    "\n",
    "\n",
    "def analyze_sentiment(review_text):\n",
    "    \"\"\"Analyze the sentiment of a review and classify it as positive or negative.\"\"\"\n",
    "    prompt = (\n",
    "        f\"Analyze the sentiment of the following review and classify it as either 'positive' or 'negative'. \"\n",
    "        f\"Provide a brief reason for your classification. Review: {review_text}\"\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    return response\n",
    "\n",
    "# Process reviews and categorize them\n",
    "def process_reviews(file_path):\n",
    "    \"\"\"Process reviews to classify them into positive and negative categories.\"\"\"\n",
    "    reviews = load_reviews(file_path)\n",
    "    positive_comments = []\n",
    "    negative_comments = []\n",
    "\n",
    "    for review in reviews:\n",
    "        sentiment_analysis = analyze_sentiment(review)\n",
    "\n",
    "        if \"positive\" in sentiment_analysis.lower():\n",
    "            positive_comments.append({\"review\": review, \"analysis\": sentiment_analysis})\n",
    "        elif \"negative\" in sentiment_analysis.lower():\n",
    "            negative_comments.append({\"review\": review, \"analysis\": sentiment_analysis})\n",
    "\n",
    "    return positive_comments, negative_comments\n",
    "\n",
    "\n",
    "def save_to_json(positive_comments, negative_comments, output_file=\"sentiment_reviews.json\"):\n",
    "    \"\"\"Save categorized reviews to a JSON file.\"\"\"\n",
    "    data = {\n",
    "        \"positive_comments\": positive_comments,\n",
    "        \"negative_comments\": negative_comments\n",
    "    }\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "input_file = \"reviewsData.json\" \n",
    "positive_comments, negative_comments = process_reviews(input_file)\n",
    "save_to_json(positive_comments, negative_comments)\n",
    "print(\"Sentiment analysis completed and saved to sentiment_reviews.json.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
